{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying meaning in autogenerated text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By: Erik Börve, Goncalo Gomes, Marcus Henriksbø"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "\n",
    "from data_helpers import *\n",
    "from tf_idf_helpers import *\n",
    "from word2vec_helpers import * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileLocation = 'Data/smart-lights_close_ASR.csv'\n",
    "X_raw,y = readCSV(fileLocation,ASR = False,process = False)\n",
    "y_num,dct_y= labelCSV(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = getTFIDF(X_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y_num,test_size = 0.5)\n",
    "\n",
    "# Select model\n",
    "cls = LogisticRegression()\n",
    "cls.fit(x_train,y_train) \n",
    "\n",
    "# Evaluate performance\n",
    "y_pred = cls.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evalutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9457831325301205\n"
     ]
    }
   ],
   "source": [
    "score = cls.score(x_test,y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression()\n",
      "0.9566265060240964\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.99      0.95       132\n",
      "           1       0.93      0.96      0.95       142\n",
      "           2       0.99      0.99      0.99       138\n",
      "           3       0.98      0.86      0.91       139\n",
      "           4       0.99      0.97      0.98       150\n",
      "           5       0.95      0.96      0.95       129\n",
      "\n",
      "    accuracy                           0.96       830\n",
      "   macro avg       0.96      0.96      0.96       830\n",
      "weighted avg       0.96      0.96      0.96       830\n",
      "\n",
      "SVC()\n",
      "0.9542168674698795\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.90      0.93       147\n",
      "           1       0.88      0.97      0.92       139\n",
      "           2       0.98      0.99      0.99       141\n",
      "           3       0.99      0.90      0.94       132\n",
      "           4       0.99      0.99      0.99       142\n",
      "           5       0.93      0.98      0.95       129\n",
      "\n",
      "    accuracy                           0.95       830\n",
      "   macro avg       0.96      0.95      0.95       830\n",
      "weighted avg       0.96      0.95      0.95       830\n",
      "\n",
      "GaussianNB()\n",
      "0.7216867469879518\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.43      0.55       129\n",
      "           1       0.81      0.83      0.82       143\n",
      "           2       0.64      0.85      0.73       151\n",
      "           3       0.87      0.78      0.83       133\n",
      "           4       0.70      0.90      0.79       154\n",
      "           5       0.56      0.46      0.50       120\n",
      "\n",
      "    accuracy                           0.72       830\n",
      "   macro avg       0.73      0.71      0.70       830\n",
      "weighted avg       0.73      0.72      0.71       830\n",
      "\n",
      "MLPClassifier(hidden_layer_sizes=(8, 8, 8), max_iter=5000)\n",
      "0.9265060240963855\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.87      0.87       139\n",
      "           1       0.95      0.92      0.94       134\n",
      "           2       0.97      0.97      0.97       153\n",
      "           3       0.92      0.91      0.91       139\n",
      "           4       0.96      0.97      0.97       140\n",
      "           5       0.87      0.92      0.89       125\n",
      "\n",
      "    accuracy                           0.93       830\n",
      "   macro avg       0.93      0.93      0.93       830\n",
      "weighted avg       0.93      0.93      0.93       830\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "n = 1\n",
    "F1_mean = np.zeros((n,len(dct_y)))\n",
    "\n",
    "classifiers = {'LogisticRegression': LogisticRegression(), 'SVM': SVC(), 'GaussianNB': GaussianNB(), 'MLPClassifier': MLPClassifier(hidden_layer_sizes=(400,100), activation='relu', solver='adam', max_iter=5000) }\n",
    "classifier_score = dict.fromkeys(classifiers.keys(),[])\n",
    "for key, value in classifiers.items():\n",
    "    for i in range(n):\n",
    "        x_train, x_test, y_train, y_test = train_test_split(X,y_num,test_size = 0.5)\n",
    "        classifier = value\n",
    "        classifier.fit(x_train,y_train)\n",
    "\n",
    "        # Evaluate performance\n",
    "        y_pred = classifier.predict(x_test)\n",
    "        print(classifier)\n",
    "        score = classifier.score(x_test,y_test)\n",
    "        print(score)\n",
    "        print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Issue for: X= 26 with word \" seventythree \".\n",
      "Word ignored in feature construction.\n",
      "Issue for: X= 97 with word \" seventythree \".\n",
      "Word ignored in feature construction.\n",
      "Issue for: X= 178 with word \" seventythree \".\n",
      "Word ignored in feature construction.\n",
      "Issue for: X= 232 with word \"  \".\n",
      "Word ignored in feature construction.\n",
      "Issue for: X= 248 with word \"  \".\n",
      "Word ignored in feature construction.\n",
      "Issue for: X= 264 with word \" seventythree \".\n",
      "Word ignored in feature construction.\n",
      "Issue for: X= 304 with word \" seventythree \".\n",
      "Word ignored in feature construction.\n",
      "Issue for: X= 400 with word \" tubelight \".\n",
      "Word ignored in feature construction.\n",
      "Issue for: X= 404 with word \" seventythree \".\n",
      "Word ignored in feature construction.\n",
      "Issue for: X= 410 with word \" seventythree \".\n",
      "Word ignored in feature construction.\n",
      "Issue for: X= 425 with word \"  \".\n",
      "Word ignored in feature construction.\n",
      "Issue for: X= 428 with word \"  \".\n",
      "Word ignored in feature construction.\n",
      "Issue for: X= 443 with word \"  \".\n",
      "Word ignored in feature construction.\n",
      "Issue for: X= 452 with word \"  \".\n",
      "Word ignored in feature construction.\n",
      "Issue for: X= 507 with word \"  \".\n",
      "Word ignored in feature construction.\n",
      "Issue for: X= 509 with word \"  \".\n",
      "Word ignored in feature construction.\n",
      "Issue for: X= 566 with word \" seventythree \".\n",
      "Word ignored in feature construction.\n",
      "Issue for: X= 606 with word \"  \".\n",
      "Word ignored in feature construction.\n",
      "Issue for: X= 656 with word \" brighenss \".\n",
      "Word ignored in feature construction.\n",
      "Issue for: X= 670 with word \" brighness \".\n",
      "Word ignored in feature construction.\n",
      "Issue for: X= 701 with word \" brighntess \".\n",
      "Word ignored in feature construction.\n",
      "Issue for: X= 816 with word \" cublicle \".\n",
      "Word ignored in feature construction.\n",
      "Issue for: X= 879 with word \" seventythree \".\n",
      "Word ignored in feature construction.\n",
      "Issue for: X= 899 with word \" seventythree \".\n",
      "Word ignored in feature construction.\n",
      "Issue for: X= 988 with word \" seventythree \".\n",
      "Word ignored in feature construction.\n",
      "Issue for: X= 1003 with word \" seventythree \".\n",
      "Word ignored in feature construction.\n",
      "Issue for: X= 1029 with word \"  \".\n",
      "Word ignored in feature construction.\n",
      "Issue for: X= 1109 with word \" seventythree \".\n",
      "Word ignored in feature construction.\n",
      "Issue for: X= 1116 with word \" seventythree \".\n",
      "Word ignored in feature construction.\n",
      "Issue for: X= 1120 with word \" seventythree \".\n",
      "Word ignored in feature construction.\n",
      "Issue for: X= 1152 with word \" pinkeverywhere \".\n",
      "Word ignored in feature construction.\n",
      "Issue for: X= 1187 with word \" seventythree \".\n",
      "Word ignored in feature construction.\n",
      "Issue for: X= 1383 with word \" seventythree \".\n",
      "Word ignored in feature construction.\n",
      "Issue for: X= 1426 with word \" seventythree \".\n",
      "Word ignored in feature construction.\n",
      "Issue for: X= 1433 with word \" cublicle \".\n",
      "Word ignored in feature construction.\n",
      "Issue for: X= 1474 with word \" seventythree \".\n",
      "Word ignored in feature construction.\n",
      "Issue for: X= 1501 with word \" seventythree \".\n",
      "Word ignored in feature construction.\n",
      "Issue for: X= 1523 with word \" seventythree \".\n",
      "Word ignored in feature construction.\n",
      "Issue for: X= 1627 with word \" seventythree \".\n",
      "Word ignored in feature construction.\n",
      "Issue for: X= 1645 with word \" cublicle \".\n",
      "Word ignored in feature construction.\n"
     ]
    }
   ],
   "source": [
    "X = getWord2Vec(X_raw,'gigaword-100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\erikb\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y_num,test_size = 0.5)\n",
    "\n",
    "# Select model\n",
    "cls = LogisticRegression()\n",
    "cls.fit(x_train,y_train) \n",
    "\n",
    "# Evaluate performance\n",
    "y_pred = cls.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evalutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9024096385542169\n"
     ]
    }
   ],
   "source": [
    "score = cls.score(x_test,y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7393ff1f3c30d6d9d9349cef1397e72bfb70d7ef6363d955503acb71d65a8d40"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
