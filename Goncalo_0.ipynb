{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: deeppavlov in /home/goncalo/.local/lib/python3.8/site-packages (0.17.1)\n",
      "Requirement already satisfied: prometheus-client==0.7.1 in /home/goncalo/.local/lib/python3.8/site-packages (from deeppavlov) (0.7.1)\n",
      "Requirement already satisfied: uvloop==0.14.0 in /home/goncalo/.local/lib/python3.8/site-packages (from deeppavlov) (0.14.0)\n",
      "Requirement already satisfied: numpy==1.18.0 in /home/goncalo/Anaconda3/envs/ada/lib/python3.8/site-packages (from deeppavlov) (1.18.0)\n",
      "Requirement already satisfied: sacremoses==0.0.35 in /home/goncalo/.local/lib/python3.8/site-packages (from deeppavlov) (0.0.35)\n",
      "Requirement already satisfied: Cython==0.29.14 in /home/goncalo/.local/lib/python3.8/site-packages (from deeppavlov) (0.29.14)\n",
      "Requirement already satisfied: pymorphy2==0.8 in /home/goncalo/.local/lib/python3.8/site-packages (from deeppavlov) (0.8)\n",
      "Requirement already satisfied: requests==2.22.0 in /home/goncalo/.local/lib/python3.8/site-packages (from deeppavlov) (2.22.0)\n",
      "Requirement already satisfied: pydantic==1.3 in /home/goncalo/.local/lib/python3.8/site-packages (from deeppavlov) (1.3)\n",
      "Requirement already satisfied: uvicorn==0.11.7 in /home/goncalo/.local/lib/python3.8/site-packages (from deeppavlov) (0.11.7)\n",
      "Requirement already satisfied: h5py==2.10.0 in /home/goncalo/.local/lib/python3.8/site-packages (from deeppavlov) (2.10.0)\n",
      "Requirement already satisfied: filelock==3.0.12 in /home/goncalo/.local/lib/python3.8/site-packages (from deeppavlov) (3.0.12)\n",
      "Requirement already satisfied: overrides==2.7.0 in /home/goncalo/.local/lib/python3.8/site-packages (from deeppavlov) (2.7.0)\n",
      "Requirement already satisfied: pyopenssl==19.1.0 in /home/goncalo/.local/lib/python3.8/site-packages (from deeppavlov) (19.1.0)\n",
      "Requirement already satisfied: pandas==0.25.3 in /home/goncalo/.local/lib/python3.8/site-packages (from deeppavlov) (0.25.3)\n",
      "Requirement already satisfied: pytz==2019.1 in /home/goncalo/.local/lib/python3.8/site-packages (from deeppavlov) (2019.1)\n",
      "Requirement already satisfied: rusenttokenize==0.0.5 in /home/goncalo/.local/lib/python3.8/site-packages (from deeppavlov) (0.0.5)\n",
      "Requirement already satisfied: pymorphy2-dicts-ru in /home/goncalo/.local/lib/python3.8/site-packages (from deeppavlov) (2.4.417127.4579844)\n",
      "Requirement already satisfied: ruamel.yaml==0.15.100 in /home/goncalo/.local/lib/python3.8/site-packages (from deeppavlov) (0.15.100)\n",
      "Requirement already satisfied: aio-pika==6.4.1 in /home/goncalo/.local/lib/python3.8/site-packages (from deeppavlov) (6.4.1)\n",
      "Requirement already satisfied: fastapi==0.47.1 in /home/goncalo/.local/lib/python3.8/site-packages (from deeppavlov) (0.47.1)\n",
      "Requirement already satisfied: tqdm==4.62.0 in /home/goncalo/.local/lib/python3.8/site-packages (from deeppavlov) (4.62.0)\n",
      "Requirement already satisfied: pytelegrambotapi==3.6.7 in /home/goncalo/.local/lib/python3.8/site-packages (from deeppavlov) (3.6.7)\n",
      "Requirement already satisfied: scikit-learn==0.21.2 in /home/goncalo/Anaconda3/envs/ada/lib/python3.8/site-packages (from deeppavlov) (0.21.2)\n",
      "Requirement already satisfied: scipy==1.4.1 in /home/goncalo/.local/lib/python3.8/site-packages (from deeppavlov) (1.4.1)\n",
      "Requirement already satisfied: nltk==3.4.5 in /home/goncalo/.local/lib/python3.8/site-packages (from deeppavlov) (3.4.5)\n",
      "Requirement already satisfied: click==7.1.2 in /home/goncalo/.local/lib/python3.8/site-packages (from deeppavlov) (7.1.2)\n",
      "Requirement already satisfied: yarl in /home/goncalo/.local/lib/python3.8/site-packages (from aio-pika==6.4.1->deeppavlov) (1.7.2)\n",
      "Requirement already satisfied: aiormq<4,>=3.2.0 in /home/goncalo/.local/lib/python3.8/site-packages (from aio-pika==6.4.1->deeppavlov) (3.3.1)\n",
      "Requirement already satisfied: starlette<=0.12.9,>=0.12.9 in /home/goncalo/.local/lib/python3.8/site-packages (from fastapi==0.47.1->deeppavlov) (0.12.9)\n",
      "Requirement already satisfied: six in /home/goncalo/Anaconda3/envs/ada/lib/python3.8/site-packages (from h5py==2.10.0->deeppavlov) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/goncalo/Anaconda3/envs/ada/lib/python3.8/site-packages (from pandas==0.25.3->deeppavlov) (2.8.2)\n",
      "Requirement already satisfied: pymorphy2-dicts<3.0,>=2.4 in /home/goncalo/.local/lib/python3.8/site-packages (from pymorphy2==0.8->deeppavlov) (2.4.393442.3710985)\n",
      "Requirement already satisfied: dawg-python>=0.7 in /home/goncalo/.local/lib/python3.8/site-packages (from pymorphy2==0.8->deeppavlov) (0.7.2)\n",
      "Requirement already satisfied: docopt>=0.6 in /home/goncalo/.local/lib/python3.8/site-packages (from pymorphy2==0.8->deeppavlov) (0.6.2)\n",
      "Requirement already satisfied: cryptography>=2.8 in /home/goncalo/.local/lib/python3.8/site-packages (from pyopenssl==19.1.0->deeppavlov) (36.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/goncalo/Anaconda3/envs/ada/lib/python3.8/site-packages (from requests==2.22.0->deeppavlov) (2021.10.8)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/goncalo/.local/lib/python3.8/site-packages (from requests==2.22.0->deeppavlov) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/goncalo/.local/lib/python3.8/site-packages (from requests==2.22.0->deeppavlov) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/goncalo/.local/lib/python3.8/site-packages (from requests==2.22.0->deeppavlov) (1.25.11)\n",
      "Requirement already satisfied: joblib in /home/goncalo/.local/lib/python3.8/site-packages (from sacremoses==0.0.35->deeppavlov) (1.1.0)\n",
      "Requirement already satisfied: websockets==8.* in /home/goncalo/.local/lib/python3.8/site-packages (from uvicorn==0.11.7->deeppavlov) (8.1)\n",
      "Requirement already satisfied: httptools==0.1.* in /home/goncalo/.local/lib/python3.8/site-packages (from uvicorn==0.11.7->deeppavlov) (0.1.2)\n",
      "Requirement already satisfied: h11<0.10,>=0.8 in /home/goncalo/.local/lib/python3.8/site-packages (from uvicorn==0.11.7->deeppavlov) (0.9.0)\n",
      "Requirement already satisfied: pamqp==2.3.0 in /home/goncalo/.local/lib/python3.8/site-packages (from aiormq<4,>=3.2.0->aio-pika==6.4.1->deeppavlov) (2.3.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/goncalo/.local/lib/python3.8/site-packages (from cryptography>=2.8->pyopenssl==19.1.0->deeppavlov) (1.14.6)\n",
      "Requirement already satisfied: multidict>=4.0 in /home/goncalo/.local/lib/python3.8/site-packages (from yarl->aio-pika==6.4.1->deeppavlov) (5.2.0)\n",
      "Requirement already satisfied: pycparser in /home/goncalo/.local/lib/python3.8/site-packages (from cffi>=1.12->cryptography>=2.8->pyopenssl==19.1.0->deeppavlov) (2.20)\n",
      "Requirement already satisfied: Pillow in /home/goncalo/Anaconda3/envs/ada/lib/python3.8/site-packages (8.4.0)\n",
      "Requirement already satisfied: numpy==1.18.0 in /home/goncalo/Anaconda3/envs/ada/lib/python3.8/site-packages (1.18.0)\n",
      "Requirement already satisfied: pycocotools==2.0.0 in /home/goncalo/Anaconda3/envs/ada/lib/python3.8/site-packages (2.0.0)\n",
      "Requirement already satisfied: scikit-learn==0.21.2 in /home/goncalo/Anaconda3/envs/ada/lib/python3.8/site-packages (0.21.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/goncalo/.local/lib/python3.8/site-packages (from scikit-learn==0.21.2) (1.1.0)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /home/goncalo/.local/lib/python3.8/site-packages (from scikit-learn==0.21.2) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /home/goncalo/Anaconda3/envs/ada/lib/python3.8/site-packages (from scikit-learn==0.21.2) (1.18.0)\n"
     ]
    }
   ],
   "source": [
    "#requirements\n",
    "import sys\n",
    "!{sys.executable} -m pip install --user deeppavlov\n",
    "\n",
    "\n",
    "!pip install Pillow\n",
    "!pip install --upgrade numpy==1.18.0\n",
    "!pip install pycocotools==2.0.0\n",
    "!pip install -U scikit-learn==0.21.2\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "from matplotlib.colors import is_color_like\n",
    "import numpy as np\n",
    "from deeppavlov.core.data.utils import simple_download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. LOAD DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data used to build our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data into a pandas dataframe\n",
    "data = pd.read_json('./snips/metadata.json', orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the data I selected only the columns that I found usefull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select data that is needed\n",
    "data_sentences = data[['transcript','keywords']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets see the keywords that we have and the number of apperances for each keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kitchen        193\n",
       "brightness     192\n",
       "bedroom        187\n",
       "living room    184\n",
       "increase       144\n",
       "decrease       143\n",
       "turn off       139\n",
       "turn on        139\n",
       "dtype: int64"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(data_sentences['keywords'].sum()).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be expected (in an ideal training set) that we would have keywords related to every \"user action\" in a uniforme distribution, but we can easily see that this is not the case. This data set is obviosly favoring the actions of 'SwitchLightOff','SwitchLightOn','DecreaseBrightness','IncreaseBrightness'. Missing sentences related to 'SetLightBrightness' and 'SetLightColor'. This assumption is based on the fact that for a sentence with the intension of changing the light colors, the color that we pretend to change the light (i.e red, yellow, blue...) is definitly a keyword, and we dont observe anything related to that subject in the keywords. The same goes for the set brightness action it should appear something related to the brightness level. This lack of data will affect the predictions of the model as we will observe later on the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASSOCIATE KEYWORDS TO USER ACTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I wanted to associate the keywords to a user action so i could build the model, this is not the most clean code ever nor the more efficient one, but I kinda wanted to build the model as fast as possible to see some results so i didn't put much thought into this implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31335/728371059.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_sentences['user_actions'] = 0\n",
      "/home/goncalo/.local/lib/python3.8/site-packages/pandas/core/indexing.py:494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n",
      "/home/goncalo/.local/lib/python3.8/site-packages/pandas/core/indexing.py:205: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "/tmp/ipykernel_31335/728371059.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_sentences.loc[index,'user_actions'] = 'SwitchLightOn'\n",
      "/tmp/ipykernel_31335/728371059.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_sentences.loc[index,'user_actions'] = 'SwitchLightOff'\n",
      "/tmp/ipykernel_31335/728371059.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_sentences.loc[index,'user_actions'] = 'DecreaseBrightness'\n",
      "/tmp/ipykernel_31335/728371059.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_sentences.loc[index,'user_actions'] = 'IncreaseBrightness'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#keywords to user action\n",
    "data_sentences['user_actions'] = 0\n",
    "\n",
    "for index, rows in data_sentences.iterrows():\n",
    "    if('turn off' in  rows.keywords):\n",
    "        data_sentences.loc[index,'user_actions'] = 'SwitchLightOff'\n",
    "    elif('turn on' in  rows.keywords):\n",
    "        data_sentences.loc[index,'user_actions'] = 'SwitchLightOn'\n",
    "    elif('increase' in rows.keywords):\n",
    "        is_num_in_actions = [element.isdigit()  for element in rows.keywords]\n",
    "        if(True in is_num_in_actions):\n",
    "            data_sentences.loc[index,'user_actions'] = 'SetLightBrightness'\n",
    "        else:\n",
    "            data_sentences.loc[index,'user_actions'] = 'IncreaseBrightness'\n",
    "    elif('decrease' in rows.keywords):\n",
    "        is_num_in_actions = [element.isdigit()  for element in rows.keywords]\n",
    "        if(True in is_num_in_actions):\n",
    "            data_sentences.loc[index,'user_actions'] = 'SetLightBrightness'\n",
    "        else:\n",
    "            data_sentences.loc[index,'user_actions'] = 'DecreaseBrightness'\n",
    "    else:\n",
    "        is_color_in_actions = [is_color_like(element)  for element in rows.keywords]\n",
    "        if(True in is_color_in_actions):\n",
    "            data_sentences.loc[index,'user_actions'] = 'SetLightColor'\n",
    "        else:\n",
    "            data_sentences.loc[index,'user_actions'] = 'Nullaction'\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save data into csv file since is the input for the model\n",
    "data_sentences.to_csv('./snips/data_sentences.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. DATA SPLIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read dataset, previous saved into the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-27 03:26:29.976 WARNING in 'deeppavlov.dataset_readers.basic_classification_reader'['basic_classification_reader'] at line 112: Cannot find snips/valid.csv file\n",
      "2021-11-27 03:26:29.977 WARNING in 'deeppavlov.dataset_readers.basic_classification_reader'['basic_classification_reader'] at line 112: Cannot find snips/test.csv file\n"
     ]
    }
   ],
   "source": [
    "from deeppavlov.dataset_readers.basic_classification_reader import BasicClassificationDatasetReader\n",
    "\n",
    "# read data from particular columns of `.csv` file\n",
    "dr = BasicClassificationDatasetReader().read(\n",
    "    data_path='./snips/',\n",
    "    train='data_sentences.csv',\n",
    "    x = 'transcript',\n",
    "    y = 'user_actions'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPLIT DATA INTO TRAIN AND VALID SETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-27 03:26:30.8 INFO in 'deeppavlov.dataset_iterators.basic_classification_iterator'['basic_classification_iterator'] at line 73: Splitting field <<train>> to new fields <<['train', 'valid']>>\n"
     ]
    }
   ],
   "source": [
    "from deeppavlov.dataset_iterators.basic_classification_iterator import BasicClassificationDatasetIterator\n",
    "\n",
    "# initialize data iterator splitting `train` field to `train` and `valid` in proportion 0.8/0.2\n",
    "train_iterator = BasicClassificationDatasetIterator(\n",
    "    data=dr,\n",
    "    field_to_split='train',  # field that will be splitted\n",
    "    split_fields=['train', 'valid'],   # fields to which the fiald above will be splitted\n",
    "    split_proportions=[0.8, 0.2],  #proportions for splitting\n",
    "    split_seed=23,  # seed for splitting dataset\n",
    "    seed=42)  # seed for iteration over dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: turn on the lights for the bedroom please\n",
      "y: SwitchLightOn\n",
      "=================\n",
      "x: increase the lights for the bedroom now\n",
      "y: IncreaseBrightness\n",
      "=================\n",
      "x: increase the lights in the kitchen\n",
      "y: IncreaseBrightness\n",
      "=================\n",
      "x: increase the brightness in the bedroom\n",
      "y: IncreaseBrightness\n",
      "=================\n",
      "x: please turn off the lights in the living room\n",
      "y: SwitchLightOff\n",
      "=================\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = train_iterator.get_instances(data_type='train')\n",
    "for x, y in list(zip(x_train, y_train))[:5]:\n",
    "    print('x:', x)\n",
    "    print('y:', y)\n",
    "    print('=================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. MODEL CREATION ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all train and valid data from iterator\n",
    "x_train, y_train = train_iterator.get_instances(data_type=\"train\")\n",
    "x_valid, y_valid = train_iterator.get_instances(data_type=\"valid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 TEXT TO VECTOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get our computer to understand any text, we need to break that word down in a way that our machine can understand. Therefore I used one of the tokenized functions that deepavlov has in order match a vector to each text sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-27 03:26:30.92 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.feature_extraction.text:TfidfVectorizer from /home/goncalo/Documents/EPFL/ML/ML_course/NLP-project/Me/tfidf_v0.pkl\n",
      "2021-11-27 03:26:30.93 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "2021-11-27 03:26:30.94 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n"
     ]
    }
   ],
   "source": [
    "from deeppavlov.models.sklearn import SklearnComponent\n",
    "\n",
    "tfidf = SklearnComponent(\n",
    "    model_class=\"sklearn.feature_extraction.text:TfidfVectorizer\",\n",
    "    infer_method=\"transform\",\n",
    "    save_path='./tfidf_v0.pkl',\n",
    "    load_path='./tfidf_v0.pkl',\n",
    "    mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-27 03:26:30.123 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 109: Fitting model sklearn.feature_extraction.textTfidfVectorizer\n",
      "2021-11-27 03:26:30.132 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 241: Saving model to /home/goncalo/Documents/EPFL/ML/ML_course/NLP-project/Me/tfidf_v0.pkl\n"
     ]
    }
   ],
   "source": [
    "from deeppavlov.models.preprocessors.str_lower import str_lower\n",
    "tfidf.fit(str_lower(train_iterator.get_instances(data_type='train')[0]))\n",
    "tfidf.save()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are able to build a machine learning model to try to predict our user intentions in a given sentence. I picked a logistic regression model but we definitly need to try other models in the future to see which one give us better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-27 03:26:30.157 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.linear_model:LogisticRegression from /home/goncalo/Documents/EPFL/ML/ML_course/NLP-project/Me/logreg_v0.pkl\n",
      "2021-11-27 03:26:30.159 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.linear_model._logisticLogisticRegression loaded  with parameters\n",
      "2021-11-27 03:26:30.160 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n"
     ]
    }
   ],
   "source": [
    "from deeppavlov.metrics.accuracy import sets_accuracy\n",
    "\n",
    "# initialize sklearn classifier, all parameters for classifier could be passed\n",
    "cls = SklearnComponent(\n",
    "    model_class=\"sklearn.linear_model:LogisticRegression\",\n",
    "    infer_method=\"predict\",\n",
    "    save_path='./logreg_v0.pkl',\n",
    "    load_path='./logreg_v0.pkl',\n",
    "    C=1,\n",
    "    mode='train')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-27 03:26:30.230 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 109: Fitting model sklearn.linear_model._logisticLogisticRegression\n",
      "2021-11-27 03:26:30.258 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 241: Saving model to /home/goncalo/Documents/EPFL/ML/ML_course/NLP-project/Me/logreg_v0.pkl\n"
     ]
    }
   ],
   "source": [
    "# fit sklearn classifier and save it\n",
    "cls.fit(tfidf(x_train), y_train)\n",
    "cls.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_pred = cls(tfidf(x_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text sample: increase lights for the kitchen now\n",
      "True label: IncreaseBrightness\n",
      "Predicted label: IncreaseBrightness\n"
     ]
    }
   ],
   "source": [
    "# Let's look into obtained result\n",
    "print(\"Text sample: {}\".format(x_valid[0]))\n",
    "print(\"True label: {}\".format(y_valid[0]))\n",
    "print(\"Predicted label: {}\".format(y_valid_pred[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sets_accuracy(np.squeeze(y_valid), y_valid_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This accuracy is unrealisticly high as expected given the fact that the sentences in the valid sample were way to similar to the training test. This result totally supports my initial assumption for our lack of variety in the dataset, and we can confirm this by self testing sentences and seeing what user action it gave us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test for Increasing/Decreasing Lights and for TurnOn/Off Lights \n",
      "\n",
      "Test1: ['IncreaseBrightness']\n",
      "Test2: ['DecreaseBrightness']\n",
      "Test3: ['SwitchLightOff']\n",
      "Test4: ['SwitchLightOn']\n",
      "\n",
      " Test for Set Brigthness/Change color \n",
      "\n",
      "Test5: ['DecreaseBrightness']\n",
      "Test6: ['DecreaseBrightness']\n",
      "Test7: ['IncreaseBrightness']\n",
      "Test8: ['DecreaseBrightness']\n"
     ]
    }
   ],
   "source": [
    "#Test for Increasing/Decreasing Lights and for TurnOn/Off Lights\n",
    "print('\\n Test for Increasing/Decreasing Lights and for TurnOn/Off Lights \\n')\n",
    "print(\"Test1: {}\".format(cls(tfidf(['Marcus needs more brightness']))))\n",
    "print(\"Test2: {}\".format(cls(tfidf(['Erick screamed: Decrease the lights in rolex']))))\n",
    "print(\"Test3: {}\".format(cls(tfidf(['Gonçalo turn off the brigtness']))))\n",
    "print(\"Test4: {}\".format(cls(tfidf(['DarthVader fliped on the brigtness']))))\n",
    "\n",
    "#Test for Set Brigthness/Change color\n",
    "print('\\n Test for Set Brigthness/Change color \\n')\n",
    "print(\"Test5: {}\".format(cls(tfidf(['Gonçalo tried to change the light to red']))))\n",
    "print(\"Test6: {}\".format(cls(tfidf(['Erick switched the light to blue']))))\n",
    "print(\"Test7: {}\".format(cls(tfidf(['Marcus set brightness to 8']))))\n",
    "print(\"Test8: {}\".format(cls(tfidf(['Yoda set the lights to 2']))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OVERALL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **We need to improve our training dataset**: Not just find/ask for more variety on the data but also find a way to augment our currrent data in order that our model fits better synonyms of the keywords (ie. increase, grow, amplify...), although the model is working well for sentences that are related to ('IncreaseBrightness','DecreaseBrightness','SwitchLightOff' amd 'SwitchLightOn') we still need to improve the model to do a better fitting in synonyms.\n",
    "\n",
    " - **We need to test more models**\n",
    "\n",
    " - **Improve the way we coorelate the keywords to the user actions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
