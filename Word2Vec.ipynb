{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import gensim.downloader\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from statistics import mean\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import _helpers as hp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "For chosing which ASR data change the variable by the key in the paths dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining paths for our data. \"ASR\" means the TDNNF-LFMMI method, and \"new_ASR\" means the Wave2vec method.\n",
    "paths = {\n",
    "    \"ground_truth\": \"snips/merged_GT_data.csv\",\n",
    "    \"ASR\": \"snips/ASR_with_labels.csv\",\n",
    "    \"new_ASR\": \"snips/new_ASR_with_labels.csv\",\n",
    "    \"new_ASR_Autocorrect\": \"snips/new_ASR_Autocorrection_with_labels.csv\",\n",
    "}\n",
    "\n",
    "# Chosing which ASR data we will use\n",
    "ASR_data = pd.read_csv(paths[\"new_ASR\"])\n",
    "\n",
    "# Groundtruth data\n",
    "GT_data = pd.read_csv(paths[\"ground_truth\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ASR data into a numpy word array per sentence plus a numpy label array\n",
    "\n",
    "XX_ASR = ASR_data[\"transcript\"].apply(lambda x: x.split(\" \"))\n",
    "X_ASR = list(XX_ASR)  # numpy word array per transcript\n",
    "\n",
    "y_num_ASR = np.array(ASR_data[\"user_action_num\"])  # labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Groundtruth data into a numpy word array per sentence plus a numpy label array\n",
    "\n",
    "XX_GT = GT_data[\"transcript\"].apply(lambda x: x.split(\" \"))\n",
    "X_GT = list(XX_GT)  # numpy word array per transcript\n",
    "\n",
    "y_num_GT = np.array(GT_data[\"user_action_num\"])  # labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import pre-trained Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v = gensim.downloader.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Word2Vec_feature(sentence):\n",
    "    \"\"\"return word2vec numpy array representation of sentence\"\"\"\n",
    "\n",
    "    value_iter = np.zeros((model_w2v.vector_size,))\n",
    "\n",
    "    for word in sentence:\n",
    "        try:\n",
    "            word_vec = model_w2v[word]\n",
    "            value_iter += np.array(word_vec)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    return value_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features Space\n",
    "features_ASR = np.stack(XX_ASR.apply(get_Word2Vec_feature).values, axis=0)\n",
    "\n",
    "# features Space\n",
    "features_GT = np.stack(XX_GT.apply(get_Word2Vec_feature).values, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_ASR = ASR_data[\"user_action_num\"]\n",
    "labels_GT = GT_data[\"user_action_num\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function beneath provides the features and labels needed for testing. Using the loaded ASR or not (then using ground truth data) is decided by input. As standard we use the ASR dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_data(type_of_dataset=\"ASR\", train_size=0.9):\n",
    "    \"\"\"Retrieves the relevant dataset and splits according to parameter\"\"\"\n",
    "    # If ASR, give ASR features and labels\n",
    "    if type_of_dataset == \"ASR\":\n",
    "        train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "            features_ASR, labels_ASR, train_size=train_size\n",
    "        )\n",
    "    # If the dataset is not the ASR data, use the ground truth data\n",
    "    else:\n",
    "        train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "            features_GT, labels_GT, train_size=train_size\n",
    "        )\n",
    "\n",
    "    return train_features, test_features, train_labels, test_labels\n",
    "\n",
    "\n",
    "def run_classifier(\n",
    "    classifier_pipe, type_of_dataset=\"ASR\", train_size=0.9, number_of_times=100\n",
    "):\n",
    "    \"\"\"For running the classifiers multiple times, and returning mean accuracy score. Wraps around get_train_test_data\"\"\"\n",
    "    mean_score_list = []\n",
    "    n = number_of_times\n",
    "    for i in range(n):\n",
    "        train_features, test_features, train_labels, test_labels = get_train_test_data(\n",
    "            type_of_dataset=\"ASR\"\n",
    "        )\n",
    "        classifier_pipe.fit(train_features, train_labels)\n",
    "\n",
    "        classifier_pred_labels = classifier_pipe.predict(test_features)  # predictions\n",
    "\n",
    "        classifier_score = classifier_pipe.score(test_features, test_labels)  # accuracy\n",
    "\n",
    "        mean_score_list.append(classifier_score)\n",
    "    mean_score = mean(mean_score_list)\n",
    "    return mean_score, classifier_pred_labels, classifier_score, test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy score = 0.783\n"
     ]
    }
   ],
   "source": [
    "lgr = LogisticRegression(C=0.06, max_iter=1000)  # Create the classification model\n",
    "\n",
    "lgr_pipe = make_pipeline(preprocessing.StandardScaler(), lgr)  # Scale feature space\n",
    "\n",
    "mean_score, lgr_pred_labels, lgr_score, test_labels = run_classifier(\n",
    "    classifier_pipe=lgr_pipe,\n",
    "    type_of_dataset=\"ASR\",\n",
    "    train_size=0.9,\n",
    "    number_of_times=100,\n",
    ")\n",
    "\n",
    "print(\"Average accuracy score =\", round(mean_score, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "Gives detail on the last run of the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "    SwitchLightOff       0.78      0.56      0.65        25\n",
      "     SwitchLightOn       0.79      0.90      0.84        30\n",
      "IncreaseBrightness       0.66      0.88      0.75        24\n",
      "DecreaseBrightness       0.76      0.69      0.72        32\n",
      "SetLightBrightness       0.90      0.96      0.93        27\n",
      "     SetLightColor       0.83      0.71      0.77        28\n",
      "\n",
      "          accuracy                           0.78       166\n",
      "         macro avg       0.79      0.78      0.78       166\n",
      "      weighted avg       0.79      0.78      0.78       166\n",
      "\n",
      "[[14  2  4  3  1  1]\n",
      " [ 0 27  2  0  0  1]\n",
      " [ 1  0 21  1  1  0]\n",
      " [ 1  2  5 22  0  2]\n",
      " [ 0  0  0  1 26  0]\n",
      " [ 2  3  0  2  1 20]]\n",
      "\n",
      "ACCURACY: 0.7831325301204819\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    classification_report(\n",
    "        test_labels,\n",
    "        lgr_pred_labels,\n",
    "        target_names=[\n",
    "            \"SwitchLightOff\",\n",
    "            \"SwitchLightOn\",\n",
    "            \"IncreaseBrightness\",\n",
    "            \"DecreaseBrightness\",\n",
    "            \"SetLightBrightness\",\n",
    "            \"SetLightColor\",\n",
    "        ],\n",
    "    )\n",
    ")\n",
    "\n",
    "print(confusion_matrix(test_labels, lgr_pred_labels))\n",
    "\n",
    "print(\"\\nACCURACY:\", lgr_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy score = 0.524\n"
     ]
    }
   ],
   "source": [
    "gnb = GaussianNB()  # Create the classification model\n",
    "\n",
    "gnb_pipe = make_pipeline(preprocessing.StandardScaler(), gnb)  # Scale feature space\n",
    "\n",
    "mean_score, gnb_pred_labels, gnb_score, test_labels = run_classifier(\n",
    "    classifier_pipe=gnb_pipe,\n",
    "    type_of_dataset=\"ASR\",\n",
    "    train_size=0.9,\n",
    "    number_of_times=100,\n",
    ")\n",
    "\n",
    "print(\"Average accuracy score =\", round(mean_score, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "    SwitchLightOff       0.52      0.45      0.48        29\n",
      "     SwitchLightOn       0.53      0.66      0.58        32\n",
      "IncreaseBrightness       0.32      0.42      0.36        19\n",
      "DecreaseBrightness       0.38      0.23      0.29        26\n",
      "SetLightBrightness       0.57      0.75      0.65        28\n",
      "     SetLightColor       0.78      0.56      0.65        32\n",
      "\n",
      "          accuracy                           0.52       166\n",
      "         macro avg       0.52      0.51      0.50       166\n",
      "      weighted avg       0.53      0.52      0.52       166\n",
      "\n",
      "[[13  5  2  2  7  0]\n",
      " [ 1 21  3  1  4  2]\n",
      " [ 1  5  8  4  1  0]\n",
      " [ 6  2  7  6  2  3]\n",
      " [ 2  0  4  1 21  0]\n",
      " [ 2  7  1  2  2 18]]\n",
      "\n",
      "ACCURACY: 0.5240963855421686\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    classification_report(\n",
    "        test_labels,\n",
    "        gnb_pred_labels,\n",
    "        target_names=[\n",
    "            \"SwitchLightOff\",\n",
    "            \"SwitchLightOn\",\n",
    "            \"IncreaseBrightness\",\n",
    "            \"DecreaseBrightness\",\n",
    "            \"SetLightBrightness\",\n",
    "            \"SetLightColor\",\n",
    "        ],\n",
    "    )\n",
    ")\n",
    "\n",
    "print(confusion_matrix(test_labels, gnb_pred_labels))\n",
    "\n",
    "print(\"\\nACCURACY:\", gnb_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy score = 0.837\n"
     ]
    }
   ],
   "source": [
    "svm = SVC()  # Create the classification model\n",
    "\n",
    "svm_pipe = make_pipeline(preprocessing.Normalizer(), svm)  # Scale feature space\n",
    "\n",
    "mean_score, svm_pred_labels, svm_score, test_labels = run_classifier(\n",
    "    classifier_pipe=svm_pipe,\n",
    "    type_of_dataset=\"ASR\",\n",
    "    train_size=0.9,\n",
    "    number_of_times=100,\n",
    ")\n",
    "\n",
    "print(\"Average accuracy score =\", round(mean_score, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "    SwitchLightOff       0.79      0.60      0.68        25\n",
      "     SwitchLightOn       0.81      0.97      0.88        30\n",
      "IncreaseBrightness       0.80      0.84      0.82        19\n",
      "DecreaseBrightness       0.77      0.77      0.77        26\n",
      "SetLightBrightness       0.94      0.94      0.94        35\n",
      "     SetLightColor       0.87      0.84      0.85        31\n",
      "\n",
      "          accuracy                           0.84       166\n",
      "         macro avg       0.83      0.83      0.82       166\n",
      "      weighted avg       0.84      0.84      0.83       166\n",
      "\n",
      "[[15  4  1  2  2  1]\n",
      " [ 0 29  1  0  0  0]\n",
      " [ 1  0 16  2  0  0]\n",
      " [ 1  2  1 20  0  2]\n",
      " [ 0  0  1  0 33  1]\n",
      " [ 2  1  0  2  0 26]]\n",
      "\n",
      "ACCURACY: 0.8373493975903614\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    classification_report(\n",
    "        test_labels,\n",
    "        svm_pred_labels,\n",
    "        target_names=[\n",
    "            \"SwitchLightOff\",\n",
    "            \"SwitchLightOn\",\n",
    "            \"IncreaseBrightness\",\n",
    "            \"DecreaseBrightness\",\n",
    "            \"SetLightBrightness\",\n",
    "            \"SetLightColor\",\n",
    "        ],\n",
    "    )\n",
    ")\n",
    "\n",
    "print(confusion_matrix(test_labels, svm_pred_labels))\n",
    "\n",
    "print(\"\\nACCURACY:\", svm_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy score = 0.783\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(400, 100), activation=\"relu\", solver=\"adam\", max_iter=5000\n",
    ")  # Create the classification model\n",
    "\n",
    "mlp_pipe = make_pipeline(preprocessing.Normalizer(), mlp)  # Scale feature space\n",
    "\n",
    "mean_score, mlp_pred_labels, mlp_score, test_labels = run_classifier(\n",
    "    classifier_pipe=mlp_pipe,\n",
    "    type_of_dataset=\"ASR\",\n",
    "    train_size=0.9,\n",
    "    number_of_times=100,\n",
    ")\n",
    "\n",
    "print(\"Average accuracy score =\", round(mean_score, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "    SwitchLightOff       0.83      0.76      0.79        33\n",
      "     SwitchLightOn       0.89      0.89      0.89        28\n",
      "IncreaseBrightness       0.72      0.72      0.72        29\n",
      "DecreaseBrightness       0.62      0.82      0.71        22\n",
      "SetLightBrightness       0.87      0.80      0.83        25\n",
      "     SetLightColor       0.96      0.90      0.93        29\n",
      "\n",
      "          accuracy                           0.81       166\n",
      "         macro avg       0.82      0.81      0.81       166\n",
      "      weighted avg       0.82      0.81      0.82       166\n",
      "\n",
      "[[25  1  3  3  1  0]\n",
      " [ 1 25  0  1  0  1]\n",
      " [ 2  0 21  6  0  0]\n",
      " [ 1  0  2 18  1  0]\n",
      " [ 1  1  3  0 20  0]\n",
      " [ 0  1  0  1  1 26]]\n",
      "\n",
      "ACCURACY: 0.8132530120481928\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    classification_report(\n",
    "        test_labels,\n",
    "        mlp_pred_labels,\n",
    "        target_names=[\n",
    "            \"SwitchLightOff\",\n",
    "            \"SwitchLightOn\",\n",
    "            \"IncreaseBrightness\",\n",
    "            \"DecreaseBrightness\",\n",
    "            \"SetLightBrightness\",\n",
    "            \"SetLightColor\",\n",
    "        ],\n",
    "    )\n",
    ")\n",
    "\n",
    "print(confusion_matrix(test_labels, mlp_pred_labels))\n",
    "\n",
    "print(\"\\nACCURACY:\", mlp_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try Your Self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_friendly(sentence, cls):\n",
    "    \"\"\"return action from sentence\"\"\"\n",
    "\n",
    "    sent = sentence.split()\n",
    "    new_sent = []\n",
    "\n",
    "    for word in sent:\n",
    "        new_sent.append(hp.autocorrection(word))\n",
    "\n",
    "    x_pred = get_Word2Vec_feature(new_sent).reshape(1, -1)\n",
    "    y_pred = cls.predict(x_pred)\n",
    "    return hp.indx2action(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SetLightBrightness']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_friendly(\"I want to set brithenss to fifty in my living room\", lgr_pipe)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "798da7fa2e3db0aae4b339803b26a00c5ccc95905add6d158a3bf74c8d303e34"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
