{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers as ppb  # pytorch transformers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from statistics import mean\n",
    "\n",
    "import _helpers as hp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "For chosing which ASR data change the variable by the key in the paths dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining paths for our data. \"ASR\" means the TDNNF-LFMMI method, and \"new_ASR\" means the Wave2vec method.\n",
    "paths = {\n",
    "    \"ground_truth\": \"snips/merged_GT_data.csv\",\n",
    "    \"ASR\": \"snips/ASR_with_labels.csv\",\n",
    "    \"new_ASR\": \"snips/new_ASR_with_labels.csv\",\n",
    "    \"new_ASR_Autocorrect\": \"snips/new_ASR_Autocorrection_with_labels.csv\",\n",
    "}\n",
    "\n",
    "# Chosing which ASR data we will use\n",
    "ASR_data = pd.read_csv(paths[\"new_ASR\"])\n",
    "\n",
    "# Groundtruth data\n",
    "GT_data = pd.read_csv(paths[\"ground_truth\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import pre-trained DistilBERT model and tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DistilBERT is a small, fast, cheap and light Transformer model. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT's performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class, tokenizer_class, pretrained_weights = (\n",
    "    ppb.DistilBertModel,\n",
    "    ppb.DistilBertTokenizer,\n",
    "    \"distilbert-base-uncased\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import pre-trained BERT model and tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to use BERT instead of DistilBERT we comment the previous model importation and use this one instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-small-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained model/tokenizer\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_GT_data = GT_data[\"transcript\"].apply(\n",
    "    (lambda x: tokenizer.encode(x, add_special_tokens=True))\n",
    ")\n",
    "\n",
    "tokenized_ASR_data = ASR_data[\"transcript\"].apply(\n",
    "    (lambda x: tokenizer.encode(x, add_special_tokens=True))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem now is that the tokenized vectors ar not with the name size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MX: 20\n",
      "MIN: 4\n"
     ]
    }
   ],
   "source": [
    "if max(tokenized_GT_data.apply(len)) > max(tokenized_ASR_data.apply(len)):\n",
    "    _max = max(tokenized_GT_data.apply(len))\n",
    "    _min = min(tokenized_GT_data.apply(len))\n",
    "else:\n",
    "    _max = max(tokenized_ASR_data.apply(len))\n",
    "    _min = min(tokenized_ASR_data.apply(len))\n",
    "\n",
    "print(\"MX:\", _max)\n",
    "print(\"MIN:\", _min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets fix that with a simple padding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1660, 20)\n",
      "(1660, 20)\n"
     ]
    }
   ],
   "source": [
    "padded_GT_data = np.array(\n",
    "    list(tokenized_GT_data.apply(lambda x: hp.padding_func(x, _max)))\n",
    ")\n",
    "padded_ASR_data = np.array(\n",
    "    list(tokenized_ASR_data.apply(lambda x: hp.padding_func(x, _max)))\n",
    ")\n",
    "print(padded_GT_data.shape)\n",
    "print(padded_ASR_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we directly send padded to BERT, that would slightly confuse it. We need to create another variable to tell it to ignore (mask) the padding we've added when it's processing its input. That's what attention_mask is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask_GT = np.where(padded_GT_data != 0, 1, 0)\n",
    "\n",
    "attention_mask_ASR = np.where(padded_ASR_data != 0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing with DistrilBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_GT = torch.tensor(padded_GT_data)\n",
    "attention_mask_GT = torch.tensor(attention_mask_GT)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states_GT = model(input_ids_GT, attention_mask=attention_mask_GT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_ASR = torch.tensor(padded_ASR_data)\n",
    "attention_mask_ASR = torch.tensor(attention_mask_ASR)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states_ASR = model(input_ids_ASR, attention_mask=attention_mask_ASR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's slice only the part of the output that we need. That is the output corresponding the first token of each sentence. The way BERT does sentence classification, is that it adds a token called [CLS] (for classification) at the beginning of every sentence\n",
    "\n",
    "basically [CLS] contains all information of the sentence and representing the sentence-level classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1660, 768)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Taking the CLS features of each sentence\n",
    "features_GT = last_hidden_states_GT[0][:, 0, :].numpy()\n",
    "features_GT.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1660, 768)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Taking the CLS features of each sentence\n",
    "features_ASR = last_hidden_states_ASR[0][:, 0, :].numpy()\n",
    "features_ASR.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_GT = GT_data[\"user_action_num\"]\n",
    "labels_ASR = ASR_data[\"user_action_num\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function beneath provides the features and labels needed for testing. Using the loaded ASR or not (then using ground truth data) is decided by input. As standard we use the ASR dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_data(type_of_dataset=\"ASR\", train_size=0.9):\n",
    "    \"\"\"Retrieves the relevant dataset and splits according to parameter\"\"\"\n",
    "    # If ASR, give ASR features and labels\n",
    "    if type_of_dataset == \"ASR\":\n",
    "        train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "            features_ASR, labels_ASR, train_size=train_size\n",
    "        )\n",
    "    # If the dataset is not the ASR data, use the ground truth data\n",
    "    else:\n",
    "        train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "            features_GT, labels_GT, train_size=train_size\n",
    "        )\n",
    "\n",
    "    return train_features, test_features, train_labels, test_labels\n",
    "\n",
    "\n",
    "def run_classifier(\n",
    "    classifier_pipe, type_of_dataset=\"ASR\", train_size=0.9, number_of_times=100\n",
    "):\n",
    "    \"\"\"For running the classifiers multiple times, and returning mean accuracy score. Wraps around get_train_test_data\"\"\"\n",
    "    mean_score_list = []\n",
    "    n = number_of_times\n",
    "    for i in range(n):\n",
    "        train_features, test_features, train_labels, test_labels = get_train_test_data(\n",
    "            type_of_dataset=\"ASR\"\n",
    "        )\n",
    "        classifier_pipe.fit(train_features, train_labels)\n",
    "\n",
    "        classifier_pred_labels = classifier_pipe.predict(test_features)  # predictions\n",
    "\n",
    "        classifier_score = classifier_pipe.score(test_features, test_labels)  # accuracy\n",
    "\n",
    "        mean_score_list.append(classifier_score)\n",
    "    mean_score = mean(mean_score_list)\n",
    "    return mean_score, classifier_pred_labels, classifier_score, test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy score = 0.777\n"
     ]
    }
   ],
   "source": [
    "lgr = LogisticRegression(\n",
    "    C=0.6, max_iter=1000, penalty=\"l2\", solver=\"liblinear\"\n",
    ")  # Create the classification model\n",
    "\n",
    "lgr_pipe = make_pipeline(preprocessing.StandardScaler(), lgr)  # Scale feature space\n",
    "\n",
    "mean_score, lgr_pred_labels, lgr_score, test_labels = run_classifier(\n",
    "    classifier_pipe=lgr_pipe,\n",
    "    type_of_dataset=\"ASR\",\n",
    "    train_size=0.9,\n",
    "    number_of_times=100,\n",
    ")\n",
    "\n",
    "print(\"Average accuracy score =\", round(mean_score, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "    SwitchLightOff       0.79      0.68      0.73        34\n",
      "     SwitchLightOn       0.71      0.77      0.74        22\n",
      "IncreaseBrightness       0.81      0.79      0.80        28\n",
      "DecreaseBrightness       0.74      0.68      0.71        25\n",
      "SetLightBrightness       0.84      0.90      0.87        30\n",
      "     SetLightColor       0.74      0.85      0.79        27\n",
      "\n",
      "          accuracy                           0.78       166\n",
      "         macro avg       0.77      0.78      0.77       166\n",
      "      weighted avg       0.78      0.78      0.78       166\n",
      "\n",
      "[[23  4  3  3  1  0]\n",
      " [ 1 17  1  1  1  1]\n",
      " [ 1  0 22  1  0  4]\n",
      " [ 1  2  1 17  2  2]\n",
      " [ 1  0  0  1 27  1]\n",
      " [ 2  1  0  0  1 23]]\n",
      "\n",
      "ACCURACY: 0.7771084337349398\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    classification_report(\n",
    "        test_labels,\n",
    "        lgr_pred_labels,\n",
    "        target_names=[\n",
    "            \"SwitchLightOff\",\n",
    "            \"SwitchLightOn\",\n",
    "            \"IncreaseBrightness\",\n",
    "            \"DecreaseBrightness\",\n",
    "            \"SetLightBrightness\",\n",
    "            \"SetLightColor\",\n",
    "        ],\n",
    "    )\n",
    ")\n",
    "\n",
    "print(confusion_matrix(test_labels, lgr_pred_labels))\n",
    "\n",
    "print(\"\\nACCURACY:\", lgr_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy score = 0.608\n"
     ]
    }
   ],
   "source": [
    "gnb = GaussianNB()  # Create the classification model\n",
    "\n",
    "gnb_pipe = make_pipeline(preprocessing.StandardScaler(), gnb)  # Scale feature space\n",
    "\n",
    "mean_score, gnb_pred_labels, gnb_score, test_labels = run_classifier(\n",
    "    classifier_pipe=gnb_pipe,\n",
    "    type_of_dataset=\"ASR\",\n",
    "    train_size=0.9,\n",
    "    number_of_times=100,\n",
    ")\n",
    "\n",
    "print(\"Average accuracy score =\", round(mean_score, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "    SwitchLightOff       0.80      0.30      0.43        27\n",
      "     SwitchLightOn       0.39      0.76      0.51        25\n",
      "IncreaseBrightness       0.71      0.52      0.60        23\n",
      "DecreaseBrightness       0.32      0.29      0.30        21\n",
      "SetLightBrightness       0.79      0.83      0.81        36\n",
      "     SetLightColor       0.79      0.76      0.78        34\n",
      "\n",
      "          accuracy                           0.61       166\n",
      "         macro avg       0.63      0.58      0.57       166\n",
      "      weighted avg       0.66      0.61      0.60       166\n",
      "\n",
      "[[ 8 12  0  4  1  2]\n",
      " [ 1 19  0  3  2  0]\n",
      " [ 0  5 12  3  1  2]\n",
      " [ 0  9  3  6  2  1]\n",
      " [ 0  1  0  3 30  2]\n",
      " [ 1  3  2  0  2 26]]\n",
      "\n",
      "ACCURACY: 0.608433734939759\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    classification_report(\n",
    "        test_labels,\n",
    "        gnb_pred_labels,\n",
    "        target_names=[\n",
    "            \"SwitchLightOff\",\n",
    "            \"SwitchLightOn\",\n",
    "            \"IncreaseBrightness\",\n",
    "            \"DecreaseBrightness\",\n",
    "            \"SetLightBrightness\",\n",
    "            \"SetLightColor\",\n",
    "        ],\n",
    "    )\n",
    ")\n",
    "\n",
    "print(confusion_matrix(test_labels, gnb_pred_labels))\n",
    "\n",
    "print(\"\\nACCURACY:\", gnb_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy score = 0.729\n"
     ]
    }
   ],
   "source": [
    "svm = SVC()  # Create the classification model\n",
    "\n",
    "svm_pipe = make_pipeline(preprocessing.StandardScaler(), svm)  # Scale feature space\n",
    "\n",
    "mean_score_list = []\n",
    "\n",
    "mean_score, svm_pred_labels, svm_score, test_labels = run_classifier(\n",
    "    classifier_pipe=svm_pipe,\n",
    "    type_of_dataset=\"ASR\",\n",
    "    train_size=0.9,\n",
    "    number_of_times=100,\n",
    ")\n",
    "\n",
    "print(\"Average accuracy score =\", round(mean_score, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "    SwitchLightOff       0.61      0.76      0.68        25\n",
      "     SwitchLightOn       0.57      0.57      0.57        28\n",
      "IncreaseBrightness       0.87      0.50      0.63        26\n",
      "DecreaseBrightness       0.67      0.73      0.70        22\n",
      "SetLightBrightness       0.82      0.93      0.87        30\n",
      "     SetLightColor       0.85      0.83      0.84        35\n",
      "\n",
      "          accuracy                           0.73       166\n",
      "         macro avg       0.73      0.72      0.72       166\n",
      "      weighted avg       0.74      0.73      0.73       166\n",
      "\n",
      "[[19  3  0  1  2  0]\n",
      " [ 6 16  0  3  2  1]\n",
      " [ 3  5 13  2  1  2]\n",
      " [ 2  1  2 16  0  1]\n",
      " [ 0  0  0  1 28  1]\n",
      " [ 1  3  0  1  1 29]]\n",
      "\n",
      "ACCURACY: 0.7289156626506024\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    classification_report(\n",
    "        test_labels,\n",
    "        svm_pred_labels,\n",
    "        target_names=[\n",
    "            \"SwitchLightOff\",\n",
    "            \"SwitchLightOn\",\n",
    "            \"IncreaseBrightness\",\n",
    "            \"DecreaseBrightness\",\n",
    "            \"SetLightBrightness\",\n",
    "            \"SetLightColor\",\n",
    "        ],\n",
    "    )\n",
    ")\n",
    "\n",
    "print(confusion_matrix(test_labels, svm_pred_labels))\n",
    "\n",
    "print(\"\\nACCURACY:\", svm_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy score = 0.751\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(400, 100), activation=\"relu\", solver=\"adam\", max_iter=5000\n",
    ")  # Create the classification model\n",
    "\n",
    "mlp_pipe = make_pipeline(preprocessing.StandardScaler(), mlp)  # Scale feature space\n",
    "\n",
    "mean_score, mlp_pred_labels, mlp_score, test_labels = run_classifier(\n",
    "    classifier_pipe=mlp_pipe,\n",
    "    type_of_dataset=\"ASR\",\n",
    "    train_size=0.9,\n",
    "    number_of_times=100,\n",
    ")\n",
    "\n",
    "print(\"Average accuracy score =\", round(mean_score, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "    SwitchLightOff       0.68      0.71      0.69        24\n",
      "     SwitchLightOn       0.48      0.71      0.57        17\n",
      "IncreaseBrightness       0.70      0.75      0.72        28\n",
      "DecreaseBrightness       0.56      0.37      0.44        27\n",
      "SetLightBrightness       0.86      0.86      0.86        36\n",
      "     SetLightColor       0.78      0.74      0.76        34\n",
      "\n",
      "          accuracy                           0.70       166\n",
      "         macro avg       0.68      0.69      0.68       166\n",
      "      weighted avg       0.70      0.70      0.70       166\n",
      "\n",
      "[[17  3  2  2  0  0]\n",
      " [ 1 12  0  0  2  2]\n",
      " [ 1  3 21  2  0  1]\n",
      " [ 4  5  5 10  0  3]\n",
      " [ 1  1  1  1 31  1]\n",
      " [ 1  1  1  3  3 25]]\n",
      "\n",
      "ACCURACY: 0.6987951807228916\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    classification_report(\n",
    "        test_labels,\n",
    "        mlp_pred_labels,\n",
    "        target_names=[\n",
    "            \"SwitchLightOff\",\n",
    "            \"SwitchLightOn\",\n",
    "            \"IncreaseBrightness\",\n",
    "            \"DecreaseBrightness\",\n",
    "            \"SetLightBrightness\",\n",
    "            \"SetLightColor\",\n",
    "        ],\n",
    "    )\n",
    ")\n",
    "\n",
    "print(confusion_matrix(test_labels, mlp_pred_labels))\n",
    "\n",
    "print(\"\\nACCURACY:\", mlp_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try your self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_friendly(sentence, mdl):\n",
    "    \"\"\"return action from sentence\"\"\"\n",
    "    sent_token = tokenizer.encode(sentence, add_special_tokens=True)\n",
    "    sent_pad = np.array([sent_token + [0] * (_max - len(sent_token))])\n",
    "    sent_att_mask = np.where(sent_pad != 0, 1, 0)\n",
    "    sent_input_ids = torch.tensor(sent_pad)\n",
    "    sent_attention_mask = torch.tensor(sent_att_mask)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        sent_last_hidden_states = model(\n",
    "            sent_input_ids, attention_mask=sent_attention_mask\n",
    "        )\n",
    "\n",
    "    feature = sent_last_hidden_states[0][:, 0, :].numpy()\n",
    "\n",
    "    prediction = mdl.predict(feature)\n",
    "    user_action = hp.indx2action(prediction)\n",
    "    return user_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SetLightColor']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_friendly(\"red light to the room\", lgr_pipe)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
