
# Finding meaning in autogenerated text
 ## Team Members

 Erik Börve: erik.borve@epfl.ch  
 Gonçalo Gomes: goncalo.cavacogomes@epfl.ch  
 Marcus Henriksbø: marcus.henriksboe@epfl.ch  

 ## Purpose
 This project focuses on spoken language understanding, aiming to derive the intent of a user's queries. The overarching goal is to help a conversational agent to respond appropriately to a user. We then evaluate the performance and the impact of using automatically generated transcripts by an audio speech recognition engine on methods of intent classification.

 ## Getting Started

 ### Prerequisites

 This project requires jupyter notebook. If not installed, look [here](https://github.com/jupyter/notebook/blob/master/README.md)

 Clone the project in your local machine, and uncompress all the datasets contained in /snips repository.

 To run the *Wav2Vec.ipynp* file, download this zip file in the [Drive folder]( https://drive.google.com/file/d/170xq6fUiAzCWNe16BbOCDUxFUwEJDtCy/view?usp=sharing) and uncompress it inside the snips folder.

Locate the repo and run:
* pip
  ```sh
  pip install -r requirements.txt
  ```


## Usage

_Each notebook corresponds to a distinct task. Three notebooks correspond to running each of the three text embeddings and their classifiers. These are the Bert, tfIdf, and Word2Vec notebooks._


1. Locate repository.
2. Run jupyter notebook.
   ```
   jupyter notebook
   ```
3. Navigate to the notebook of interest. If wanting to run classifiers using Bert, go to Bert.ipynb, etc. The different notebooks are described below in the project structure section.


 ## Project Structure

```bash
.
├── Bert.ipynb
├── Data/
│   └── openvoc-keyword-spotting-research-datasets/
│       └── smart-lights/
│           ├── metadata.json
│           ├── smart-lights_close.csv
│           └── smart_speaker_close.csv
├── Handling_data.ipynb
├── README.md
├── TfIdf.ipynb
├── Wav2vec.ipynb
├── Word2Vec.ipynb
├── __pycache__/
│   └── _helpers.cpython-38.pyc
├── _helpers.py
├── requirements.txt
└── snips/
    ├── merged_GT_data.csv
    ├── metadata.json
    ├── new_ASR_Autocorrection_with_labels.csv
    ├── new_ASR_Autocorrection_without_labels.csv
    ├── new_ASR_with_labels.csv
    ├── new_ASR_without_labels.csv
    ├── old_ASR_data.csv
    ├── smart-lights_close_ASR.csv
    └── smart_speaker_close_ASR.csv

```

 In *Bert.ipynb* one can find the normal classification task execution using Bert's natural language processing techniques to generate the feature space.

 In *TfIdf.ipynb* one can find the normal classification task execution using TfIdf's natural language processing techniques to generate the feature space.

 In *Word2Vec.ipynb* one can find the normal classification task execution using Word2Vec's natural language processing techniques to generate the feature space.

 In *Wav2vec.ipynb* one can find the Audio speech recognition engine used to produce the automatically generated transcripts.

 In *Handling_Data.ipynb* one can find the dataset manipulation using pandas dataframes to produce datasets properly fitted to use on the natural language processing technics and on the classification tasks.

 In *_helpers.py* one can find the helper functions for the normal execution of the project.


 ### For Speech Recognition (Wav2vec.ipynb)

 #### Loading data
 Load .wav files

 #### Loading models
 Load facebook's pre-trained speech recognition models

 #### Audio files properties check
 Check the sampling rate of audio files

 #### Audio speech recognition engine
 Transform the audio into string

 #### Auto-correction
  Do an individual word autocorrection for each word present in ASR transcripts for the most similar word present in the Groundtruth word vocabulary.
 #### Save dataframe
  Save ASR data into a .csv file

 ### For Data manipulation (Handling_Data.ipynb)

 #### Loading data
  Load data set
 #### Label data
  Label each transcript to the corresponding intent index

 ### Normal Execution
  This structure is repeated on every classification task files

 #### Loading Data
  Load datasets properly fitted to use in the natural language processing methods and classification tasks

 #### Feature Space generation
  Produce the feature space using the proper Natural Language Processing Technique

 #### Split into test/train
  Split data into train and test samples to use in the classification tasks

 #### Generate Predictions
  Generate predictions for each classifier

 #### Model Evaluation
  Evaluate each classifier model using the proper metrics

 #### Try your self
  The final function in the file focuses to try to catch the user intent in new made-up sentences with the NLP technique used in the file and a given classifier.
